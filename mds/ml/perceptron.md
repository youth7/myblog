# 感知机
感知机是一种**线性分类模型**，可以用来做**二类分类**。它的思路是构造出一个超平面，用这个超平面来将输入空间的实例划分为两类。以下是《统计学习方法——感知机学习方法》一章笔记。

## 模型
$$
f(x)= sign(w^T x +  b)
$$
$$
\begin{equation}
   sign(y) = 
   \begin{cases}
   +1& y \geq 0\\
   -1& y < 0
   \end{cases}
\end{equation}
$$
$x$：输入空间的实例（一个$n$维列向量）。   
$w$：权值向量（一个$n$维列向量），$w$用来标记组成$x$的每个元素的权重。   
$b$：一个实数。

根据参考资料1，设$w =(w_1,w_2...w_n )$，点$p = (p_1,p_2...p_n ), x = (x_1,x_2...x_n )$分别是空间中的一定点和动点，则$w^T \vec{px} = 0$描述了经过点$p$，并且以$w$为法向量的点的集合（即超平面），代入具体坐标可得
$$
\begin{aligned}
w^T \vec{px} &= w_1(x_1-p_1)+w_2(x_2-p_2)...w_n(x_n-p_n) \\
&= w_1x_1+w_2x_2...w_nx_n - w_1p_1+w_2p_2...w_np_n  \\
&= w^T x + b \\
\end{aligned}
$$
其中$b=w_1p_1+w_2p_2...w_np_n$。所以感知机的模型其实就是描述了一个**经过点$p$并且以$w$为法向量的超平面**。   

$w^T x +  b$是一个线性二分器，它的直观意义是：数据$x$具有$n$个特征（即$x=(x_1, x_2..., x_n)$），线性二分器通过**给每个特征赋予不同的权重，加权计算出一个值，并以此作为分类的依据**，整个过程也可以看做是特征值的线性组合。

进行分类其实有多种模型，为何选择最为简单的线性模型呢，我个人觉得有以下几种可能：
* 现实中某些事物的分类依据就是特征值得线性组合（例如学生总成绩计算，期末50%，期中30%， 平时20%）
* 线性模型虽是最简单的模型，但可以复合其它函数使其变为非线性模型（例如逻辑斯蒂回归）
* 根据奥卡姆剃刀原则，如果不太清楚什么函数可以将数据分类的时候，不妨先使用最简单的线性分类器。



## 策略
一个良好的超平面在分类过程中应该产生尽可能少的误分类点，如果将这些误分类点到$S$的距离加起来，则这个总和也应该尽可能小，这就是感知机的学习策略。  
误分类点到超平面的距离为 $\frac{-1}{||w||}y_i(w^T x_i + b)$（推理过程见参考资料），则总和（即损失函数）为：
$$L(w, b) = -\sum_{x_i∈M} =  y_i(w^T x_i + b)$$ 可以看出损失函数故意忽略了$\frac{-1}{||w||}$，在一些教材中并没有给出相关解释，从直观上看忽略的部分对函数的影响应该不大。

## 算法
制定好策略之后，问题转变为在输入空间求下$L(w, b)$的最小值。按照相关教材，这里使用梯度下降法进行求解。$L(w, b)$的梯度为：
$$
∇_wL(w,b) = -\sum_{x_i∈M}y_ix_i \\
∇_bL(w,b) = -\sum_{x_i∈M}y_i
$$
这里需要强调几点：
* 此时函数的自变量是$w$和$b$
* **完整的梯度函数需要将全部样本$(x_i, y_i)$代入，然后获得一个只关于$w$或$b$的函数**

然而和一般的梯度下降法不同，感知机的梯度下降法（按照李航的《统计学习法方法》）有很多让人困惑的地方：
* 它不是一次过将所有的误分类点$M$一次过全部代入（因为一开始无法获取全部误分类点），而是发现一个误分类点就进行一次梯度下降（可以认为此时误分类点的样本空间只有一个元素）并更新$w$和$b$的值。  
* 当发现一个误分类的时候，它对$w$和$b$的更新也不是像常规那样不断递归直到梯度接近0为止，而是只更新一次就停止，直到碰到下一个误分类点才进行新一次的更新

我个人觉得整个推理过程是非常牵强的，好像只是为了跟梯度下降法扯上关系而硬套上（为了跟其它算法保持一致性的原因？），从上面的分析可以看出感知机的梯度下降跟一般的梯度下降有很大区别，甚至不能说是梯度下降法了。    

我觉得应该这样分析：我们的目标是使得$L$最小化，什么时候它最小，当然就是误分类点为0的时候最小了。那怎么使得误分类点消失？思路就是碰到误分类点的时候微调$w$和$b$，这样平面会产生一些倾斜和位移，从而将这个误分类点包含进去超平面的另一侧。那么怎么微调$w$和$b$？台大林轩田老师的《機器學習基石》中关于感知器的一章给予了提示，如下图所示：  
![](/imgs/perceptron0.jpg)   
>注意：林轩田老师的课程中通过一些技巧将$b$统一到损失函数中，因此他的损失函数中没有$b$，因此只需要调整$w$即可。

* 当算得$wx + b <0$ 但是实际上$y=1$的时候  
此时$x$和$w$之间的夹角过大，因此需要减少两者之间的夹角，令$w = w + ηyx$，则更新后的$w$和$x$之间的夹角变为小   
* 当算得$wx + b >0$ 但是实际上$y=-1$的时候，如下图所示  
此时$x$和$w$之间的夹角过小，因此需要增加两者之间的夹角，令$w = w + ηyx$，则更新后的$w$和$x$之间的夹角变为大  

可见无论误分类点向哪边偏离，$wx + b <0$都可以用来表示修正后的结果，而$η$能控制学习的速率，意思是当碰到不符合要求的点的时候，$w$需要以多大的幅度调整自身。

以上我个人的理解，感觉比硬套梯度下降法更加容易接受。



## 一些不足
* 现实中很多数据并不一定是线性可分。
* 如果退而求次，将策略改为求“误分类点最少超平面”，则这是一个NP问题

## 参考文章
* [谈谈超平面（hyperplane）](https://www.cnblogs.com/xiangshancuizhu/archive/2011/09/08/2171029.html)
* [几种范数的简单介绍](https://blog.csdn.net/shijing_0214/article/details/51757564)
* [空间任一点到超平面的距离公式的推导过程](https://blog.csdn.net/yutao03081/article/details/76652943)
* [机器学习笔记六之梯度下降、优化梯度公式、随机梯度下降](http://www.devtalking.com/articles/machine-learning-6/)
* [梯度下降（Gradient Descent）小结](https://www.cnblogs.com/pinard/p/5970503.html)